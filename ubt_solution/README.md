# Universal Behavioral Transformer (UBT) Solution

## 1. Project Overview

This repository implements **Universal Behavioral Transformer (UBT)** – a multi-task deep learning framework that learns a single user representation able to tackle:

1. Churn prediction  
2. Future category propensity  
3. Future product (SKU) propensity  
4. Future purchase price regression  
5. Next-purchase product-name vector prediction

After training, the model can embed every user into a fixed-size vector that serves a variety of downstream recommender-system and CRM tasks.

---

## 2. Complete Directory Layout

```text
.
├── run_churn.sh               # example: train churn only
├── run_category.sh            # example: train category propensity only
├── run_churn_category.sh      # example: jointly train churn + category
├── run_price.sh               # example: train price regression only
└── ubt_solution               # python package root (importable: `python -m ubt_solution.xxx`)
    ├── __init__.py
    ├── config.py              # global hyper-parameters & hardware flags
    ├── create_embeddings.py   # end-to-end train + inference entry-point
    ├── data_processor.py      # thin alias that re-exports dataset utilities
    ├── train.py               # simple CLI wrapper around Trainer
    ├── trainer.py             # full training / validation / inference loop
    ├── utils.py               # (empty placeholder – feel free to extend)
    ├── models                 # neural network modules
    │   ├── __init__.py
    │   ├── enhanced_feature_encoder.py
    │   ├── hstu_modules.py
    │   ├── positional_encoding.py
    │   ├── sequence_encoder.py
    │   ├── task_specific_encoder.py
    │   ├── universal_behavioral_transformer.py
    │   └── __pycache__/       # auto-generated byte-code caches
    ├── data_processing        # high-performance data pipeline
    │   ├── __init__.py
    │   ├── data_loader.py
    │   ├── dataset.py
    │   ├── memory_utils.py
    │   ├── target_data.py
    │   ├── utils.py
    │   └── __pycache__/
    └── __pycache__/           # auto-generated byte-code caches
```

> Nothing in the tree is hidden – every source file and helper directory is listed, including Python `__pycache__` folders that are normally ignored.

---

## 3. Data Layout

```
/data
 ├─ input/                         # historical behavioural logs (e.g. events.parquet)
 │   └─ relevant_clients.npy       # users participating in the task
 └─ target/                        # supervision labels
     ├─ train.parquet              # training set (client_id, churn, category, sku, price, ...)
     ├─ valid.parquet              # validation set (same schema)
     └─ active_clients.npy         # recently active users – used to soften churn loss
```

Point the scripts to a different folder with `--data-dir` if your dataset lives elsewhere.

---

## 4. Model Architecture

| Component | Description |
|-----------|-------------|
| **EnhancedFeatureEncoder** | Embeds 7 feature families – event type, category, price bucket, item ID, URL, product-name bag-of-words, search query – into a common hidden size and fuses them with an MLP + attention-style importance weighting. |
| **HSTUJagged** (from *hstu_modules.py*) | Hierarchical Sequential Transduction Unit with relative time/position bias, jagged (variable-length) batching support and key-value caching. |
| **UniversalBehavioralTransformer** | Stacks FeatureEncoder → HSTU → TaskSpecificEncoder; adds focal loss, dynamic task weights, gradient clipping, negative-sampling NCE, etc. |
| **TaskSpecificEncoder** | Independent MLP heads for each task (classification/regression) and a 16-dim vector head for the name-vector task. |

### Loss function

```math
L = Σ w_i · task_loss_i + novelty_weight · (L_{novelty\_cat} + L_{novelty\_sku})
```

* **Classification tasks** – focal loss *(α, γ)* with class-imbalance re-weighting  
* **Price regression** – MSE scaled by `price_weight`  
* **Novelty** – encourages diversity in recommended categories / SKUs  

Every coefficient is configurable via `Config.task_weights` or `--task-weights` CLI flag.

---

## 5. Training & Inference Pipeline

1. **Data loading** – `BehaviorSequenceDataset` chunks users to fit RAM and supports parallel parquet reading + hashing for dimension reduction.  
2. **Dynamic negative sampling** – each mini-batch samples fresh negative categories / SKUs online.  
3. **Forward pass** – HSTU emits per-step embeddings that are pooled (`last + mean`) into a user vector.  
4. **Multi-task loss** – compute losses, weight them and sum.  
5. **Backward pass** – Adam optimizer, gradient clipping (1.0).  
6. **Scheduler / early stopping** – `ReduceLROnPlateau` on the main task validation loss.  
7. **Inference** – generate `(client_id, embedding)` and save as `embeddings.npy`.

---

## 6. Quick Start

```bash
# Example 1 – churn-only, 3 epochs
bash run_churn.sh

# Example 2 – custom multi-task setup
python -m ubt_solution.create_embeddings \
  --data-dir /path/to/data \
  --embeddings-dir ./outputs/emb_epoch3_multi \
  --batch-size 256 \
  --num-epochs 3 \
  --learning-rate 5e-5 \
  --task-weights "churn:1.0,category_propensity:0.5,product_propensity:0.5,price:1.0,name:0.3"
```

Output files:

```
embeddings.npy   # shape: [num_users, hidden_size]
client_ids.npy   # corresponding user IDs
```

---

## 7. Key Hyper-parameters (see `config.py` for full list)

| Parameter | Default | Purpose |
|-----------|---------|---------|
| `hidden_size` | 256 | Transformer hidden dimension |
| `num_layers`  | 3   | Number of stacked HSTU layers |
| `max_seq_length` | 300 | Maximum event sequence length |
| `num_negative_samples` | 400 | Negatives per task per user |
| `focal_loss_alpha` / `gamma` | 0.75 / 2.0 | Focal-loss imbalance control |
| `learning_rate` | 5e-5 | Initial Adam LR |

---

