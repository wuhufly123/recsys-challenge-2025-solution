# Aggregated-Feature User Embedding Baseline

> A feature-engineering pipeline that generates user representations for RecSys2025 (or any similar e-commerce scenario).

---

## 1. Directory layout

```
baseline/
 ├─ aggregated_features_baseline/
 │   ├─ calculators.py          # feature calculators & interfaces
 │   ├─ constants.py            # global constants & enums
 │   ├─ create_embeddings.py    # entry script: produce & save embeddings
 │   └─ features_aggregator.py  # combine raw and derived features
 └─ README.md                  # ← this document
```

## 2. Quick start

```bash
# (1) install dependencies
pip install -r requirements.txt        # or install pandas / numpy / tqdm manually

# (2) build embeddings
python -m baseline.aggregated_features_baseline.create_embeddings \
  --data-dir       /path/to/split_data_output \
  --embeddings-dir /path/to/save_embeddings \
  --num-days       1 7 14 30 90 180 \
  --top-n          10

# You will obtain
# ├─ embeddings.npy   # shape = (N_user, D)
# └─ client_ids.npy   # line-wise aligned with embeddings
```

> **Data assumption**  The directory is generated by `data_utils.split_data` and contains `input/` and `target/`. The script also expects
> * `input/relevant_clients.npy` – list of evaluation users
> * `product_properties.parquet` – SKU ↔ quantised text embedding & category mapping
> * Event parquet files: `add_to_cart.parquet`, `product_buy.parquet`, `page_visit.parquet`, `remove_from_cart.parquet`, `search_query.parquet`
>
> Adjust paths inside `create_embeddings.py` if your layout differs.

---

## 3. Method overview

1. **Event types**
   - `product_buy`
   - `add_to_cart`
   - `remove_from_cart`
   - `page_visit`
   - `search_query`

2. **Feature hierarchy** (managed by `features_aggregator.py`)
   1. *Statistical counts*  — `StatsFeaturesCalculator`
      - for **buy / add / remove / visit**
      - count each selected column (SKU / Category / Price / URL) within multiple windows (*1,7,14,30,90,180* days)
      - keep only the top-*n* frequent values per column (default 10) to reduce sparsity
   2. *Query vectors* — `QueryFeaturesCalculator`
      - for **search_query**
      - average all historical query embeddings; dimension equals the original quantised vector (16)
   3. **Derived behavioural features** (implemented inside `FeaturesAggregator`)
      - *Recency* days since last buy/add/visit/remove (4)
      - *Frequency* event counts within 7 / 30 days (2 × 4 = 8)
      - *Entropy* category entropy in the last 30 days (1)
      - *EWA counts* exponentially weighted sums with half-lives τ ∈ {1,7,14,30,90,180}; four event types ⇒ 24 dims
      - *Behaviour ratios* five ratios based on 30-day counts (e.g. buy/visit, add/visit)
      - *Lifecycle* user age, active span, last-gap ratio (3)
   4. **Product-name vectors**
      - read quantised name embeddings from `product_properties.parquet` and average per user (16)

   After concatenation the final user vector reaches **≈ 700+ dimensions** (exact size depends on *top-n* and data stats).

3. **Normalisation**
   - Per-column z-score; high-variance columns are first `log1p` transformed
   - Block-wise L2 normalisation (see `block_ranges`) to keep sub-spaces comparable

---

## 4. Key scripts

### 4.1 `calculators.py`

* Abstract base `Calculator` with unified `features_size` and `compute_features`.
* `StatsFeaturesCalculator` → size = `(Σ selected values) × (#windows) + 1 (total count)`
* `QueryFeaturesCalculator` → simple mean, no time window involved.

### 4.2 `features_aggregator.py`

1. Choose the proper `Calculator` per event type
2. Compute additional derived features (Recency / EWA / Entropy …)
3. `merge_features` concatenates all parts and applies normalisation

### 4.3 `create_embeddings.py`

1. Load data → filter evaluation users → call `aggregator.generate_features`
2. Collect & average product-name vectors
3. Call `aggregator.merge_features` for the base embedding, then append name vector
4. Standardise + block normalise → save as `embeddings.npy`
5. Dump aligned `client_ids.npy`

---

## 5. Important parameters

| Parameter      | Default | Description |
|----------------|---------|-------------|
| `--num-days`   | `1 7 14 30 90 180` | time windows for stats & EWA |
| `--top-n`      | `10`    | keep top-n frequent values per column |
| `K`            | `5`     | size of propensity category list (currently unused) |
| `EMB_DIM`      | `16`    | name-vector dimension |

---

## 6. Dimension breakdown (example)

Example for `top-n = 10`, `num-days = 6`; actual numbers vary with data.

| Block                     | Dim |
|---------------------------|----:|
| product_buy_stats         | ~181 |
| add_to_cart_stats         | ~181 |
| remove_from_cart_stats    | ~181 |
| page_visit_stats          | ~61  |
| search_query_stats        | 16   |
| recency                   | 4    |
| behaviour_counts (7/30d)  | 8    |
| entropy                   | 1    |
| EWA counts                | 24   |
| behaviour_ratios          | 5    |
| lifecycle                 | 3    |
| product_name_embedding    | 16   |
| **Total**                 | ≈ 700 |

---

## 7. Reference

The baseline is inspired by classic RFM/FMCG user profiling and the [YouTube DNN paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf). See `features_aggregator.py` for implementation details.

---

If the README diverges from the code, or if you have questions or suggestions, feel free to open an issue or PR. Thanks! 